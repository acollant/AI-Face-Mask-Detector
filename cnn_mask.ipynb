{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb9e617-ec4f-42a4-bdf7-746fe18cce84",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e849b51f-8e04-49d9-a4d4-fa85cd49e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed73bd2-6c69-4343-a6ee-466012ba107a",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "We want to upload our data in a format that a CNN can understand. In our case our images are 128 by 128 pixels with colors. The shape of each image is $(128, 128, 3)$.\n",
    "\n",
    "```load_image``` transforms image to a ```numpy``` array of size $(128, 128, 3)$, while ```create_npy_from_image``` loops over images in a given directory and returns an array (```numpy```) of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fbad896-b32a-4fce-a096-4efde65f683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    \"\"\"This function loads an image into memory when you give it\n",
    "       the path of the image\n",
    "    \"\"\"\n",
    "    img = Image.open(infilename)\n",
    "    img.load()\n",
    "    data = np.asarray(img, dtype=\"float32\")\n",
    "    \n",
    "    if data.shape[2] == 4:\n",
    "        data = np.delete(data, 3, 2)\n",
    "        \n",
    "    return data, data.shape\n",
    "\n",
    "\n",
    "def create_npy_from_image(images_class, st, en):\n",
    "    \"\"\"Loops through the images in a folder and saves all of them\n",
    "       as a numpy array in output_name\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    print(\"##############################################\")\n",
    "    print(f\"class: {images_class}\")\n",
    "    for i in range(st, en+1):\n",
    "        img, shape = load_image(f'./data/{images_class}/{images_class}{i}.png')\n",
    "        data.append(img)\n",
    "        if (i-1)%50 == 0:\n",
    "            print(f\"image number {i}: shape is {shape}\")\n",
    "    \n",
    "    # filename.endswith(\".jpg\"):\n",
    "    return np.array(data, dtype=np.object_)\n",
    "    # return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29021967-301a-41e7-91ec-55bd8f165aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "class: cloth\n",
      "image number 1: shape is (128, 128, 3)\n",
      "image number 51: shape is (128, 128, 3)\n",
      "image number 101: shape is (128, 128, 3)\n",
      "image number 151: shape is (128, 128, 3)\n",
      "image number 201: shape is (128, 128, 3)\n",
      "##############################################\n",
      "class: cloth\n",
      "image number 251: shape is (128, 128, 3)\n",
      "##############################################\n",
      "class: no_mask\n",
      "image number 1: shape is (128, 128, 3)\n",
      "image number 51: shape is (128, 128, 3)\n",
      "image number 101: shape is (128, 128, 3)\n",
      "image number 151: shape is (128, 128, 3)\n",
      "image number 201: shape is (128, 128, 3)\n",
      "##############################################\n",
      "class: no_mask\n",
      "image number 251: shape is (128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = 250, 50\n",
    "img_classes = [\"cloth\", \"no_mask\"]\n",
    "train_data = []\n",
    "test_data = []\n",
    "for img_cl in img_classes:\n",
    "    train_data.append(create_npy_from_image(img_cl, 1, train_set))\n",
    "    test_data.append(create_npy_from_image(img_cl, train_set+1, train_set+test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "493c0862-3d49-4071-8e42-c010fa402689",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.Tensor(np.concatenate(train_data, axis=0).astype(np.int32))\n",
    "train_targets = torch.Tensor(np.concatenate(([0 for i in range(train_set)], [1 for i in range(train_set)]), axis=0).astype(np.int32)).type(torch.IntTensor)\n",
    "\n",
    "test_features = torch.Tensor(np.concatenate(test_data, axis=0).astype(np.int32))\n",
    "test_targets = torch.Tensor(np.concatenate(([0 for i in range(test_set)], [1 for i in range(test_set)]), axis=0).astype(np.int32)).type(torch.IntTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a744b-0f1c-41bd-ada4-8f08ac29edf1",
   "metadata": {},
   "source": [
    "Last modification to our dataset is using ```data_utils.TensorDataset``` to transform our dataset to a [map-style datasets](https://pytorch.org/docs/stable/data.html#:~:text=of%20these%20options.-,Dataset%20Types,-The%20most%20important). This is useful because it automatically generates batches in the training loop and takes care of shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "222ea356-9205-48ae-8fa8-5c2ca9bee04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(train_features, train_targets)\n",
    "test_data = data_utils.TensorDataset(test_features, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea0389-a950-4b27-98af-06307c35695c",
   "metadata": {},
   "source": [
    "# Simple NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe38f00-89f7-407a-a67c-e8b1279d703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(49152, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "603a16ce-6df8-43b9-a77e-20cb5c8e6123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=49152, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "286be269-3c0a-4342-8bd8-b7f5080cff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d13d7886-84a2-4831-ac5f-5f278929b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3a6ab3a-a72b-464f-b2ed-69719566e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=1, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a7050-b13b-4364-85b7-659c9f63eb49",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "058a69bb-02ac-4e85-b09f-4e144baccd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 5.6762, Test Loss: 2.0194\n",
      "Epoch 20/100, Train Loss: 2.2675, Test Loss: 2.2650\n",
      "Epoch 30/100, Train Loss: 2.2335, Test Loss: 2.2310\n",
      "Epoch 40/100, Train Loss: 2.2001, Test Loss: 2.1976\n",
      "Epoch 50/100, Train Loss: 2.1677, Test Loss: 2.1654\n",
      "Epoch 60/100, Train Loss: 2.1362, Test Loss: 2.1340\n",
      "Epoch 70/100, Train Loss: 2.1055, Test Loss: 2.1033\n",
      "Epoch 80/100, Train Loss: 2.0754, Test Loss: 2.0733\n",
      "Epoch 90/100, Train Loss: 2.0458, Test Loss: 2.0438\n",
      "Epoch 100/100, Train Loss: 2.0168, Test Loss: 2.0149\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# Stuff to store\n",
    "train_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "\n",
    "for it in range(n_epochs):\n",
    "    train_loss = []\n",
    "    for inputs, targets in train_loader:\n",
    "        # move data to GPU\n",
    "        inputs, targets = inputs.to(device), targets.to(device).type(torch.LongTensor)\n",
    "\n",
    "        # reshape the input\n",
    "        inputs = inputs.view(-1, 49152)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss) # a little misleading\n",
    "  \n",
    "    test_loss = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device).type(torch.LongTensor)\n",
    "        inputs = inputs.view(-1, 49152)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss.append(loss.item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "    \n",
    "    if (it+1)%10 == 0:\n",
    "        print(f'Epoch {it+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64267f79-a3f5-47c5-ad9c-0a8bb097f689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the train loss and test loss per iteration\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4553f-4089-44c0-9e19-3d446301d497",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebde4808-6206-4ae7-bf42-31f14f457f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "#Â K = len(set(train_data.targets.numpy()))\n",
    "K = 2\n",
    "print(\"number of classes:\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7edf0faf-2dce-4dfe-8077-f9e1c25938cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "        # http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n",
    "        # \"No zero padding, non-unit strides\"\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "        self.dense_layers = nn.Sequential(\n",
    "          nn.Dropout(0.2),\n",
    "          nn.Linear(128 * 2 * 2, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.2),\n",
    "          nn.Linear(512, K)\n",
    "        )\n",
    "  \n",
    "    def forward(self, X):\n",
    "        out = self.conv_layers(X)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense_layers(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1adf997e-2903-45f8-87fe-6e0f7a755888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = CNN(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e40dce03-c3ea-4b34-b138-1ad9f54138bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (dense_layers): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e53ca21a-53a3-427c-bebe-baf10b5bed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e51426e-fa1e-443d-ab6f-7a40f6043965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# Useful because it automatically generates batches in the training loop\n",
    "# and takes care of shuffling\n",
    "\n",
    "batch_size = 250\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29d1f58a-3d41-491a-8577-d450fe9eb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "\n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "  \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c86074c-3c2f-4670-8093-e65bb6dd9699",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[128, 128, 128, 3] to have 3 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_gd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36mbatch_gd\u001b[0;34m(model, criterion, optimizer, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_layers(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[128, 128, 128, 3] to have 3 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = batch_gd(\n",
    "    model, criterion, optimizer, train_loader, test_loader, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1395502-5dfc-492f-9fa0-3fca40286996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
